# â™Ÿï¸ Chess AI â€“ MLP from Scratch & Minimax

> **MÃ´n há»c:** TrÃ­ tuá»‡ NhÃ¢n táº¡o (HCMUT)  
> **BÃ i táº­p lá»›n:** Game Playing â€“ Cá» vua
---

## 1. Má»¥c tiÃªu cá»§a project

Dá»± Ã¡n hiá»‡n thá»±c má»™t AI chÆ¡i cá» vua vá»›i 2 hÆ°á»›ng tiáº¿p cáº­n:

1. **Minimax Agent**  
   - Thuáº­t toÃ¡n Minimax + cáº¯t tá»‰a Alphaâ€“Beta  
   - ÄÃ¡nh giÃ¡ tháº¿ cá» báº±ng **giÃ¡ trá»‹ quÃ¢n + báº£ng Ä‘iá»ƒm vá»‹ trÃ­ (Piece-Square Tables)**  

2. **MLP Agent (Neural Network)**   
   - Há»— trá»£ **GPU (NVIDIA)** qua `CuPy`, náº¿u khÃ´ng cÃ³ GPU sáº½ tá»± Ä‘á»™ng cháº¡y báº±ng CPU (`NumPy`)

Trong bÃ¡o cÃ¡o, nhÃ³m cÃ³ thá»ƒ **so sÃ¡nh**:

- Minimax (thuáº­t toÃ¡n cá»• Ä‘iá»ƒn)  
- MLP (há»c tá»« dá»¯ liá»‡u)  
- Random (baseline ngáº«u nhiÃªn)

---

## 2. Cáº¥u trÃºc thÆ° má»¥c

```text
BTL2_NMAI/
â”œâ”€â”€ agents/                 # CÃ¡c "Ä‘áº¥u thá»§" (Agents)
â”‚   â”œâ”€â”€ minimax_agent.py    # Minimax + Alphaâ€“Beta + PST
â”‚   â”œâ”€â”€ mlp_agent.py        # Agent dÃ¹ng model MLP
â”‚   â””â”€â”€ random_agent.py     # Agent Ä‘Ã¡nh ngáº«u nhiÃªn (baseline)
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ model_mlp.py        # MLP tá»« scratch: Linear, ReLU, Tanh, SGD
â”‚   â”œâ”€â”€ train_mlp.py        # Script train / fine-tune MLP
â”‚   â”œâ”€â”€ prepare_data_hf.py  # Script chuáº©n bá»‹ dá»¯ liá»‡u tá»« file parquet â†’ CSV
â”‚   â”œâ”€â”€ dataset_large.csv   # (Sinh ra sau khi cháº¡y prepare_data_hf.py)
â”‚   â””â”€â”€ best_model_mlp.npz  # Trá»ng sá»‘ model Ä‘Ã£ train
â”œâ”€â”€ utils.py                # HÃ m chuyá»ƒn board â†’ tensor (13 x 8 x 8)
â”œâ”€â”€ main.py                 # Cháº¿ Ä‘á»™ chÆ¡i tá»«ng vÃ¡n
â”œâ”€â”€ benchmark.py            # Benchmark nhiá»u vÃ¡n láº¥y thá»‘ng kÃª win-rate
â”œâ”€â”€ requirements.txt        # CÃ¡c thÆ° viá»‡n cáº§n cÃ i
```

---

## 3. CÃ i Ä‘áº·t mÃ´i trÆ°á»ng (Windows â€“ khuyáº¿n nghá»‹)

> **YÃªu cáº§u:**  
> - ÄÃ£ cÃ i **Python 3.11**

### 3.1. Má»Ÿ PowerShell trong thÆ° má»¥c project

- Giáº£i nÃ©n project  
- Chuá»™t pháº£i trong thÆ° má»¥c `BTL2_NMAI` â†’ **Open in Terminal** hoáº·c má»Ÿ PowerShell vÃ  `cd` Ä‘áº¿n thÆ° má»¥c Ä‘Ã³.

### 3.2. Táº¡o vÃ  kÃ­ch hoáº¡t virtualenv

**BÆ°á»›c A â€“ Cho phÃ©p cháº¡y script (táº¡m thá»i cho phiÃªn nÃ y - náº¿u cháº¡y trÃªn powershell thÃ¬ má»›i dÃ¹ng cÃ²n cháº¡y trÃªn cmd thÃ¬ khÃ´ng cáº§n):**

```powershell
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
```

**BÆ°á»›c B â€“ Táº¡o mÃ´i trÆ°á»ng áº£o Python 3.11:**

```powershell
py -3.11 -m venv venv-3.11
```

**BÆ°á»›c C â€“ KÃ­ch hoáº¡t mÃ´i trÆ°á»ng áº£o:**

```powershell
.venv-3.11\Scripts\activate
```

> Náº¿u thÃ nh cÃ´ng, Ä‘áº§u dÃ²ng lá»‡nh sáº½ cÃ³ `(venv-3.11)`.

### 3.3. CÃ i thÆ° viá»‡n phá»¥ thuá»™c

```powershell
pip install -r requirements.txt
```

## LÆ°u Ã½ khi muá»‘n dÃ¹ng GPU ##
Náº¿u muá»‘n sá»­ dá»¥ng GPU thÃ¬ cáº§n cÃ i cupy vÃ  cuda toolkit trÆ°á»›c.
Cháº¡y lá»‡nh nvidia-smi Ä‘á»ƒ kiá»ƒm tra phiÃªn báº£n driver GPU hiá»‡n táº¡i sau Ä‘Ã³ cÃ i cupy tÆ°Æ¡ng á»©ng.
NÃªn cáº­p nháº­t lÃªn driver má»›i nháº¥t, khi Ä‘Ã³ dÃ¹ng lá»‡nh nÃ y Ä‘á»ƒ táº£i:
```powershell
pip install cupy-cuda13x
```
Sau Ä‘Ã³ táº£i cuda toolkit tÆ°Æ¡ng á»©ng.

## Fix bug ##

Náº¿u bá»‹ lá»—i khÃ´ng tÃ¬m tháº¥y Ä‘Æ°á»ng dáº«n cá»§a CUDA thÃ¬ cháº¡y lá»‡nh nÃ y:


```powershell
$env:CUDA_PATH = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v13.0"
```
VÃ´ C:\Program Files\NVIDIA GPU Computing Toolkit kiáº¿m folder CUDA xem nÃ³ cÃ³ folder phiÃªn báº£n chÆ°a, náº¿u chÆ°a thÃ¬ táº£i láº¡i rá»“i lÃ m.
Náº¿u muá»‘n set cá»©ng Ä‘á»ƒ nhá»¯ng láº§n sau khÃ´ng cáº§n cháº¡y lá»‡nh nÃ y ná»¯a thÃ¬ táº¡o:
```powershell
New-Item -Type File -Path $PROFILE -Force     
```
náº¿u chÆ°a cÃ³, rá»“i cháº¡y:
```powershell
notepad $PROFILE 
```
Sau Ä‘Ã³ dÃ¡n lá»‡nh nÃ y $env:CUDA_PATH = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v13.0" vÃ o cuá»‘i file rá»“i lÆ°u láº¡i


### 3.4. (Tuá»³ chá»n) GPU â€“ CUDA + CuPy

- Náº¿u mÃ¡y cÃ³ **NVIDIA GPU** vÃ  Ä‘Ã£ cÃ i **CUDA Toolkit**, `CuPy` sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÄƒng tá»‘c train/inference.  
- Náº¿u khÃ´ng cÃ³ GPU: code tá»± Ä‘á»™ng dÃ¹ng `NumPy` (cháº¡y cháº­m hÆ¡n nhÆ°ng váº«n hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng).

---

## 4. Quy trÃ¬nh cháº¡y tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i

### BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u

Script sáº½ Ä‘á»c file parquet trong thÆ° má»¥c `training/` vÃ  sinh ra file CSV dÃ¹ng Ä‘á»ƒ train.

```bash
python training/prepare_data_hf.py
```

- **Káº¿t quáº£:** sinh file `training/dataset_large.csv`  
- Dá»¯ liá»‡u gá»“m cÃ¡c tháº¿ cá» (FEN) + nhÃ£n Ä‘Ã¡nh giÃ¡ `eval` trong khoáº£ng [-1, 1]:

  - 1.0  â‰ˆ tháº¿ tháº¯ng tráº¯ng  
  - -1.0 â‰ˆ tháº¿ tháº¯ng Ä‘en  

- KhÃ´ng cáº§n cháº¡y láº¡i lá»‡nh nÃ y vÃ¬ dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c táº¡o.
- Náº¿u muá»‘n cháº¡y thÃ¬ cáº§n táº£i file dá»¯ liá»‡u (.parquet) tá»« link nÃ y trÆ°á»›c vÃ  bá» vÃ o thÆ° má»¥c training: https://huggingface.co/datasets/angeluriot/chess_games

---

### BÆ°á»›c 2: Train model MLP ğŸ§ 

#### CÃ¡ch 1 â€“ Train tá»« Ä‘áº§u

DÃ¹ng cáº¥u hÃ¬nh máº·c Ä‘á»‹nh (phÃ¹ há»£p Ä‘á»ƒ cháº¡y demo vÃ  cÃ³ káº¿t quáº£ cho bÃ¡o cÃ¡o):

```bash
python training/train_mlp.py --epochs 30 --batch-size 64 --lr 0.01
```

- **Äáº§u vÃ o:** `training/dataset_large.csv`  
- **Äáº§u ra:** `training/best_model_mlp.npz` (model Ä‘Ã£ train)

#### CÃ¡ch 2 â€“ Fine-tune (nÃ¢ng cao, tuá»³ chá»n)

Náº¿u báº¡n Ä‘Ã£ cÃ³ model cÅ© vÃ  muá»‘n fine-tune thÃªm:

```bash
python training/train_mlp.py --resume-from training/best_model_mlp.npz --finetune-data training/dataset_large.csv --output-path training/best_model_mlp_finetuned.npz --epochs 10 --lr 0.001
```
---

### BÆ°á»›c 3: Benchmark â€“ sinh sá»‘ liá»‡u win-rate ğŸ“Š

ÄÃ¢y lÃ  bÆ°á»›c Ä‘á»ƒ **láº¥y sá»‘ liá»‡u cho pháº§n Ä‘Ã¡nh giÃ¡** trong BTL.

Cháº¡y:

```bash
python benchmark.py
```

TrÃªn mÃ n hÃ¬nh sáº½ hiá»‡n menu:

- Chá»n mode:
  - `1`: Minimax (Tráº¯ng) vs Random (Äen)  
  - `4`: MLP Agent vs Random  (náº¿u Ä‘Ã£ import Ä‘Æ°á»£c MLPAgent)  

Sau Ä‘Ã³ chÆ°Æ¡ng trÃ¬nh sáº½ há»i:

```text
>>> So luong van muon chay (VD: 10, 50, 100):
```

Káº¿t thÃºc, chÆ°Æ¡ng trÃ¬nh sáº½ in:

- Sá»‘ vÃ¡n tráº¯ng tháº¯ng / Ä‘en tháº¯ng / hÃ²a  
- Tá»‰ lá»‡ pháº§n trÄƒm (%) cho má»—i bÃªn  
- Káº¿t luáº­n **Äáº T / CHÆ¯A Äáº T** so vá»›i **yÃªu cáº§u bÃ i táº­p lá»›n**:

  - **Minimax vs Random:**  
    YÃªu cáº§u: *Minimax tháº¯ng â‰¥ 90%*  
  - **MLP vs Random:**  
    YÃªu cáº§u: *MLP tháº¯ng â‰¥ 60%*

---

### BÆ°á»›c 4: Cháº¡y cháº¿ Ä‘á»™ chÆ¡i Ä‘á»ƒ chá»¥p hÃ¬nh minh hoáº¡ ğŸ®

#### 4.1. Cháº¿ Ä‘á»™ terminal (CLI)

```bash
python main.py
```

- Chá»n má»™t trong cÃ¡c cháº¿ Ä‘á»™, vÃ­ dá»¥:
  - Minimax (Tráº¯ng) vs Random (Äen)  
  - MLP (Tráº¯ng) vs Minimax (Äen)

- MÃ n hÃ¬nh sáº½ in bÃ n cá» ASCII sau má»—i nÆ°á»›c Ä‘i.  

#### 4.2. Cháº¿ Ä‘á»™ GUI (náº¿u file `gui_game.py` cÃ³ trong project)

```bash
python gui_game.py
```

- Giao diá»‡n vá»›i Ä‘á»“ hoáº¡, tuy nhiÃªn nÃ³ cÃ²n khÃ¡ lÃ  Ä‘Æ¡n giáº£n vÃ  cÅ©ng cÃ²n nhiá»u lá»—i chÆ°a fix.
- Gá»“m 2 cháº¿ Ä‘á»™: ngÆ°á»i chÆ¡i Ä‘áº¥u vá»›i agent (chá»n 1 trong 3) vÃ  agent Ä‘áº¥u vá»›i agent

---

## 5. TÃ³m táº¯t ká»¹ thuáº­t

### 5.1. MLP tá»« scratch

- **Input:** tensor kÃ­ch thÆ°á»›c `13 x 8 x 8`  
  - 12 kÃªnh: loáº¡i quÃ¢n (tráº¯ng/Ä‘en)  
  - 1 kÃªnh: lÆ°á»£t Ä‘i (Tráº¯ng = 1, Äen = -1)

- **Kiáº¿n trÃºc:**
  - Flatten: 13Ã—8Ã—8 = 832 features  
  - Linear 832 â†’ 1024 + ReLU  
  - Linear 1024 â†’ 512 + ReLU  
  - Linear 512 â†’ 1 + Tanh  
  - **Äáº§u ra:** sá»‘ thá»±c trong [-1, 1] (xáº¥p xá»‰ Ä‘Ã¡nh giÃ¡ tháº¿ cá»: Ã¢m = nghiÃªng vá» Äen, dÆ°Æ¡ng = nghiÃªng vá» Tráº¯ng)

- **Huáº¥n luyá»‡n:**
  - Loss: **MSE (Mean Squared Error)**  
  - Tá»‘i Æ°u: **SGD (Gradient Descent)**

- **Backend tÃ­nh toÃ¡n:**
  - Náº¿u cÃ³ GPU + CUDA: dÃ¹ng `CuPy` (tá»‘c Ä‘á»™ cao)  
  - Náº¿u khÃ´ng: dÃ¹ng `NumPy` (tá»‘c Ä‘á»™ bÃ¬nh thÆ°á»ng)

### 5.2. Minimax Agent

- Thuáº­t toÃ¡n **Minimax + Alpha-Beta Pruning**  
- Äá»™ sÃ¢u máº·c Ä‘á»‹nh: `depth = 3`  
- HÃ m Ä‘Ã¡nh giÃ¡ (evaluation function):
  - **Material**: giÃ¡ trá»‹ quÃ¢n (Pawn, Knight, Bishop, Rook, Queen, King)  
  - **Piece-Square Tables (PST)**: má»—i quÃ¢n cÃ³ báº£ng Ä‘iá»ƒm vá»‹ trÃ­ riÃªng cho trung cuá»™c / tÃ n cuá»™c  
  - Ná»™i suy giá»¯a **Middle Game** vÃ  **End Game** dá»±a trÃªn sá»‘ lÆ°á»£ng quÃ¢n trÃªn bÃ n  

---

## 6. Káº¿t quáº£ ká»³ vá»ng

Trong Ä‘iá»u kiá»‡n train vÃ  benchmark há»£p lÃ½, ká»³ vá»ng:

| Cáº·p Ä‘áº¥u                      | Win-rate ká»³ vá»ng (tham kháº£o) |
| ---------------------------- | ----------------------------- |
| Minimax (Depth 3) vs Random | â‰¥ 90% (thÆ°á»ng ~95% trá»Ÿ lÃªn)  |
| MLP (Scratch) vs Random     | â‰¥ 60% (phá»¥ thuá»™c cháº¥t lÆ°á»£ng train) |

> **MÃ´ táº£:** Thá»­ nghiá»‡m cháº¡y **500 vÃ¡n Ä‘áº¥u liÃªn tiáº¿p** (Tournament Mode) Ä‘á»ƒ kiá»ƒm tra Ä‘á»™ á»•n Ä‘á»‹nh vÃ  tá»· lá»‡ tháº¯ng cá»§a cÃ¡c thuáº­t toÃ¡n AI so vá»›i Ä‘á»‘i thá»§ ngáº«u nhiÃªn (Random Agent).

## ğŸ† Tá»•ng há»£p So sÃ¡nh

| Agent (Phe Tráº¯ng) | Äá»‘i thá»§ (Phe Äen) | Sá»‘ vÃ¡n | Tá»· lá»‡ Tháº¯ng | Tá»‘c Ä‘á»™ trung bÃ¬nh | Káº¿t quáº£ |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Minimax (Depth 3)** | Random | 500 | **98.4%** | 2.44 s/vÃ¡n | âœ… Äáº T (>90%) |
| **MLP Agent (Scratch)** | Random | 500 | **99.0%** | **0.27 s/vÃ¡n** | âœ… Äáº T (>60%) |

---

## ğŸ“ Chi tiáº¿t Thá»­ nghiá»‡m

### 1. Minimax Agent (Alpha-Beta Pruning)
* **Cáº¥u hÃ¬nh:** Äá»™ sÃ¢u (Depth) = 3.
* **Äá»‘i thá»§:** Random Agent.
* **Thá»i gian cháº¡y:** 1218.71 giÃ¢y (~20 phÃºt).

**Káº¿t quáº£ thá»‘ng kÃª:**
* ğŸ³ï¸ **Tháº¯ng:** 492 vÃ¡n (98.4%)
* ğŸ´ **Thua:** 0 vÃ¡n (0.0%)
* ğŸ¤ **HÃ²a:** 8 vÃ¡n (1.6%)

> **Nháº­n xÃ©t:** Thuáº­t toÃ¡n Minimax hoáº¡t Ä‘á»™ng ráº¥t á»•n Ä‘á»‹nh, khÃ´ng thua vÃ¡n nÃ o. Tuy nhiÃªn, tá»‘c Ä‘á»™ tÃ­nh toÃ¡n khÃ¡ cháº­m (2.44s/nÆ°á»›c) do pháº£i duyá»‡t cÃ¢y tÃ¬m kiáº¿m.

---

### 2. MLP Agent
* **Cáº¥u hÃ¬nh:** Máº¡ng MLP (Linear -> ReLU -> Tanh), cháº¡y trÃªn GPU (CuPy).
* **Äá»‘i thá»§:** Random Agent.
* **Thá»i gian cháº¡y:** 133.85 giÃ¢y (~2 phÃºt).

**Káº¿t quáº£ thá»‘ng kÃª:**
* ğŸ³ï¸ **Tháº¯ng:** 495 vÃ¡n (99.0%)
* ğŸ´ **Thua:** 2 vÃ¡n (0.4%)
* ğŸ¤ **HÃ²a:** 3 vÃ¡n (0.6%)

### 3. MLP Agent vs Minimax Agent
* **Cáº¥u hÃ¬nh:** Máº¡ng MLP (Linear -> ReLU -> Tanh), cháº¡y trÃªn GPU (CuPy).
* **Äá»‘i thá»§:** Minimax Agent.
* **Thá»i gian cháº¡y:** 740.11 giÃ¢y.

**Káº¿t quáº£ thá»‘ng kÃª:**
* ğŸ³ï¸ **Tháº¯ng:** 0 tháº¯ng (0.0%)
* ğŸ´ **Thua:** 500 tháº¯ng (100.0%)
* ğŸ¤ **HÃ²a:** 0 vÃ¡n (0.0%)
---
## ğŸ“‰ PhÃ¢n tÃ­ch Tháº¥t báº¡i: MLP vs Minimax

> **Hiá»‡n tÆ°á»£ng:** Trong thá»­ nghiá»‡m Ä‘á»‘i Ä‘áº§u trá»±c tiáº¿p, MLP Agent (Ä‘á»™ sÃ¢u 1) thua 100% trÆ°á»›c Minimax Agent (Ä‘á»™ sÃ¢u 3).

**NguyÃªn nhÃ¢n ká»¹ thuáº­t:**
1.  **Thiáº¿u kháº£ nÄƒng tÃ¬m kiáº¿m (Lack of Search):**
    * **Minimax:** Sá»­ dá»¥ng thuáº­t toÃ¡n Alpha-Beta Pruning Ä‘á»ƒ duyá»‡t cÃ¢y trÃ² chÆ¡i vá»›i Ä‘á»™ sÃ¢u 3 (nhÃ¬n trÆ°á»›c ~3-4 nÆ°á»›c Ä‘i). NÃ³ cÃ³ thá»ƒ phÃ¡t hiá»‡n cÃ¡c báº«y chiáº¿n thuáº­t (Tactical traps) vÃ  trÃ¡nh cÃ¡c nÆ°á»›c Ä‘i sai láº§m dáº«n Ä‘áº¿n máº¥t quÃ¢n.
    * **MLP Agent:** Hoáº¡t Ä‘á»™ng theo cÆ¡ cháº¿ **Greedy (Tham lam)**. NÃ³ chá»n nÆ°á»›c Ä‘i cÃ³ Ä‘iá»ƒm sá»‘ cao nháº¥t dá»±a trÃªn tráº¡ng thÃ¡i hiá»‡n táº¡i mÃ  khÃ´ng kiá»ƒm chá»©ng háº­u quáº£ trong tÆ°Æ¡ng lai. Äiá»u nÃ y khiáº¿n MLP dá»… bá»‹ Minimax gÃ i báº«y.

2.  **Báº£n cháº¥t cá»§a hÃ m Ä‘Ã¡nh giÃ¡ (Evaluation Function):**
    * Máº¡ng MLP há»c Ä‘Æ°á»£c trá»±c giÃ¡c (Intuition) tá»« dá»¯ liá»‡u (Vá»‹ trÃ­ tá»‘t, cáº¥u trÃºc tá»‘t).
    * Tuy nhiÃªn, trong cá» vua, má»™t sai láº§m chiáº¿n thuáº­t nhá» (blunder) sáº½ phÃ¡ há»ng má»i lá»£i tháº¿ vá» vá»‹ trÃ­. Minimax lÃ  thuáº­t toÃ¡n tá»‘i Æ°u Ä‘á»ƒ trá»«ng pháº¡t cÃ¡c sai láº§m nÃ y.

**Káº¿t luáº­n:**
Máº¡ng NÆ¡-ron (MLP) lÃ m ráº¥t tá»‘t viá»‡c Ä‘Ã¡nh giÃ¡ tháº¿ cá» tá»•ng quÃ¡t (tháº¯ng Random 99%), nhÆ°ng Ä‘á»ƒ trá»Ÿ thÃ nh má»™t cao thá»§, nÃ³ cáº§n Ä‘Æ°á»£c káº¿t há»£p vá»›i thuáº­t toÃ¡n tÃ¬m kiáº¿m (Search Algorithm).
## 7. License
MIT
